#!/usr/bin/env python3
"""
AI ëª¨ë¸ ë¡œë“œ í™•ì¸ ë° ì§„ë‹¨ ìŠ¤í¬ë¦½íŠ¸
FastAPI ì„œë²„ì™€ ë…ë¦½ì ìœ¼ë¡œ ëª¨ë¸ ë¡œë“œë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""
import os
import sys

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ PYTHONPATHì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(__file__))

from ai.loader import (
    get_id2label,
    get_resolved_model_path,
    is_model_loaded,
    load_ai_model,
    resolve_model_path,
)


def check_dependencies():
    """í•„ìš”í•œ ì˜ì¡´ì„± í™•ì¸"""
    print("\n" + "=" * 60)
    print("ğŸ” ì˜ì¡´ì„± í™•ì¸")
    print("=" * 60)

    try:
        import torch
        print(f"âœ… PyTorch: {torch.__version__}")
        print(f"   - CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"   - CUDA ë²„ì „: {torch.version.cuda}")
    except ImportError as e:
        print(f"âŒ PyTorch: {e}")
        return False

    try:
        from transformers import DetaImageProcessor  # noqa: F401
        print("âœ… Transformers: ì„¤ì¹˜ë¨")
    except ImportError as e:
        print(f"âŒ Transformers: {e}")
        return False

    try:
        from PIL import Image  # noqa: F401
        print("âœ… Pillow: ì„¤ì¹˜ë¨")
    except ImportError as e:
        print(f"âŒ Pillow: {e}")
        return False

    return True


def check_model_file():
    """ëª¨ë¸ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ë° í¬ê¸° í™•ì¸"""
    print("\n" + "=" * 60)
    print("ğŸ“ ëª¨ë¸ íŒŒì¼ í™•ì¸")
    print("=" * 60)

    model_path = resolve_model_path()
    if not model_path:
        print("âŒ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. MODEL_PATH í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” server/ai í´ë”ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return False

    print(f"ëª¨ë¸ ê²½ë¡œ: {model_path}")

    if not os.path.exists(model_path):
        print("âŒ ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
        print(f"   ì˜ˆìƒ ìœ„ì¹˜: {model_path}")
        return False

    file_size = os.path.getsize(model_path)
    file_size_mb = file_size / (1024 * 1024)
    print("âœ… ëª¨ë¸ íŒŒì¼ ì¡´ì¬")
    print(f"   í¬ê¸°: {file_size_mb:.1f} MB")

    if file_size < 1024 * 1024:  # 1MB ë¯¸ë§Œ
        print("âš ï¸  ê²½ê³ : ëª¨ë¸ íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤ (ì†ìƒë˜ì—ˆì„ ìˆ˜ ìˆìŒ)")

    return True


def test_model_loading():
    """ì‹¤ì œ ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 60)
    print("ğŸš€ ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    print("ëª¨ë¸ ë¡œë”© ì¤‘...")
    success = load_ai_model()

    if success and is_model_loaded():
        print("âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")

        resolved = get_resolved_model_path()
        if resolved:
            print(f"   ì‚¬ìš©ëœ ëª¨ë¸: {resolved}")

        id2label = get_id2label()
        if id2label:
            print(f"\ní´ë˜ìŠ¤ ë ˆì´ë¸” ({len(id2label)}ê°œ):")
            for id, label in id2label.items():
                print(f"   {id}: {label}")

        return True
    else:
        print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨!")
        return False


def test_api_status():
    """FastAPI ì„œë²„ì˜ AI ëª¨ë¸ ìƒíƒœ í™•ì¸"""
    print("\n" + "=" * 60)
    print("ğŸŒ FastAPI ì„œë²„ AI ìƒíƒœ í™•ì¸")
    print("=" * 60)

    try:
        import requests
        response = requests.get("http://localhost:8080/ai/model/status", timeout=5)

        if response.status_code == 200:
            data = response.json()
            print("âœ… FastAPI ì‘ë‹µ:")
            print(f"   ìƒíƒœ: {data.get('status')}")
            print(f"   ì‚¬ìš© ê°€ëŠ¥: {data.get('available')}")

            if data.get('labels'):
                print(f"   ë ˆì´ë¸”: {data.get('labels')}")

            return data.get('available', False)
        else:
            print(f"âŒ FastAPI ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")
            return False

    except requests.RequestException as e:
        print(f"âŒ FastAPI ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
        print("   (ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”)")
        return False
    except ImportError:
        print("âš ï¸  requests ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ API í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")
        return None


def main():
    """ë©”ì¸ ì§„ë‹¨ í•¨ìˆ˜"""
    print("\n" + "=" * 60)
    print("ğŸ”§ AI ëª¨ë¸ ë¡œë“œ ì§„ë‹¨ ë„êµ¬")
    print("=" * 60)

    results = {
        "dependencies": check_dependencies(),
        "model_file": check_model_file(),
        "model_loading": False,
        "api_status": None,
    }

    if results["dependencies"] and results["model_file"]:
        results["model_loading"] = test_model_loading()

    results["api_status"] = test_api_status()

    # ìµœì¢… ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“Š ì§„ë‹¨ ìš”ì•½")
    print("=" * 60)

    all_passed = True

    if results["dependencies"]:
        print("âœ… ì˜ì¡´ì„± í™•ì¸: í†µê³¼")
    else:
        print("âŒ ì˜ì¡´ì„± í™•ì¸: ì‹¤íŒ¨")
        all_passed = False

    if results["model_file"]:
        print("âœ… ëª¨ë¸ íŒŒì¼: ì¡´ì¬")
    else:
        print("âŒ ëª¨ë¸ íŒŒì¼: ì—†ìŒ")
        all_passed = False

    if results["model_loading"]:
        print("âœ… ëª¨ë¸ ë¡œë“œ: ì„±ê³µ")
    else:
        print("âŒ ëª¨ë¸ ë¡œë“œ: ì‹¤íŒ¨")
        all_passed = False

    if results["api_status"] is True:
        print("âœ… FastAPI ìƒíƒœ: ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥")
    elif results["api_status"] is False:
        print("âš ï¸  FastAPI ìƒíƒœ: ëª¨ë¸ ë¯¸ì‚¬ìš© (ì¬ì‹œì‘ í•„ìš”)")
        all_passed = False
    else:
        print("âš ï¸  FastAPI ìƒíƒœ: í™•ì¸ ë¶ˆê°€")

    print("\n" + "=" * 60)

    if all_passed and results["api_status"] is True:
        print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! AI ê¸°ëŠ¥ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
        return 0
    else:
        print("âš ï¸  ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ìœ„ ë©”ì‹œì§€ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

        if results["model_loading"] and results["api_status"] is False:
            print("\nğŸ’¡ í•´ê²° ë°©ë²•:")
            print("   FastAPI ì„œë²„ë¥¼ ì¬ì‹œì‘í•˜ë©´ ëª¨ë¸ì´ ë¡œë“œë©ë‹ˆë‹¤:")
            print("   1. í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ: pkill -f 'uvicorn main:app'")
            print("   2. ì„œë²„ ì¬ì‹œì‘: cd /home/dbs0510/heritage_services_app_dbs0510/server")
            print("                 python3 -m uvicorn main:app --host 0.0.0.0 --port 8080")

        return 1


if __name__ == "__main__":
    exit_code = main()
