"""
AI ëª¨ë¸ ë¡œë”
ëª¨ë¸ ë¡œë”© ë° ìƒíƒœ ê´€ë¦¬
"""

import glob
import os
import pickle
import shutil
import sys
import tempfile
import traceback
import zipfile
from pathlib import Path

import torch
from transformers import DetaImageProcessor

from .model import CustomDeta

# ìƒìˆ˜ ì •ì˜
AI_DIR = Path(__file__).resolve().parent
DEFAULT_NUM_CLASSES = 4
KOREAN_LABELS = {0: "ê°ˆë¨", 1: "ê· ì—´", 2: "ë¶€í›„", 3: "ì••ê´´/í„°ì§"}
DETA_MODEL_NAME = "jozhang97/deta-resnet-50"

PREFERRED_MODEL_HINTS = [
    "hanok_damage_model.pth",
    "hanok_damage_model.pt",
    "best_model.pth",
    "best_model.pt",
    "models/hanok_damage_model.pth",
    "models/hanok_damage_model.pt",
    "models/best_model.pth",
    "models/best_model.pt",
    "hanok_damage_model_1108/best_model.pth",
    "hanok_damage_model_1108/best_model.pt",
]

# ì „ì—­ ë³€ìˆ˜
model = None
processor = None
id2label = None
id2label_korean = None
resolved_model_path = None


def _resolve_path_hint(path_hint):
    """ì§€ì •ëœ ê²½ë¡œ(íŒŒì¼ ë˜ëŠ” í´ë”)ë¥¼ ì‹¤ì œ ëª¨ë¸ íŒŒì¼ ê²½ë¡œë¡œ ë³€í™˜"""
    if not path_hint:
        return None

    candidate = Path(path_hint)
    if not candidate.is_absolute():
        candidate = AI_DIR / candidate

    if candidate.is_dir():
        return _find_best_checkpoint(str(candidate))
    if candidate.is_file():
        return str(candidate)
    return None


def _find_model_path():
    """
    ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
    1. í™˜ê²½ë³€ìˆ˜ MODEL_PATH í™•ì¸
    2. PREFERRED_MODEL_HINTS ìˆœì„œëŒ€ë¡œ ê²€ìƒ‰
    3. ê¸°ë³¸ê°’: í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ ê°€ì¥ ìµœê·¼ .pt ë˜ëŠ” .pth íŒŒì¼
    """
    env_path = os.getenv("MODEL_PATH")
    if env_path:
        resolved = _resolve_path_hint(env_path)
        if resolved:
            return resolved
        print(f"[AI] âš ï¸  í™˜ê²½ë³€ìˆ˜ MODEL_PATH ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {env_path}")

    for hint in PREFERRED_MODEL_HINTS:
        resolved = _resolve_path_hint(hint)
        if resolved:
            return resolved

    pt_files = glob.glob(str(AI_DIR / "*.pt"))
    pth_files = glob.glob(str(AI_DIR / "*.pth"))
    all_files = pt_files + pth_files

    if all_files:
        return max(all_files, key=os.path.getmtime)

    return None


def _find_best_checkpoint(model_dir):
    """í´ë” ë‚´ì—ì„œ best_mapì´ ê°€ì¥ ë†’ì€ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤"""
    print(f"[AI] ëª¨ë¸ í´ë”ì—ì„œ ìµœì  ëª¨ë¸ ê²€ìƒ‰ ì¤‘: {model_dir}")

    checkpoint_files = glob.glob(os.path.join(model_dir, "*.pth"))
    checkpoint_files.extend(glob.glob(os.path.join(model_dir, "*.pt")))

    if not checkpoint_files:
        print(f"[AI] âš ï¸  {model_dir}ì— ëª¨ë¸ íŒŒì¼(.pth/.pt)ì´ ì—†ìŠµë‹ˆë‹¤!")
        return None

    print(f"[AI] ë°œê²¬ëœ ì²´í¬í¬ì¸íŠ¸: {len(checkpoint_files)}ê°œ")

    best_checkpoint_path = None
    best_map_value = -1

    for ckpt_path in sorted(checkpoint_files):
        try:
            ckpt = torch.load(ckpt_path, map_location="cpu", weights_only=False)
            epoch = ckpt.get("epoch", -1)
            best_map = ckpt.get("best_map", -1)

            filename = os.path.basename(ckpt_path)
            epoch_str = epoch + 1 if epoch >= 0 else "N/A"
            map_str = f"{best_map:.4f}" if best_map >= 0 else "N/A"
            print(f"[AI]   ğŸ“„ {filename} - Epoch: {epoch_str}, Best mAP: {map_str}")

            if isinstance(best_map, (int, float)) and best_map > best_map_value:
                best_map_value = best_map
                best_checkpoint_path = ckpt_path

        except Exception as e:
            print(f"[AI]   âš ï¸  {os.path.basename(ckpt_path)}: ë¡œë“œ ì‹¤íŒ¨ - {e}")
            continue

    if best_checkpoint_path:
        print(
            f"[AI] âœ… ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ: {os.path.basename(best_checkpoint_path)} "
            f"(Best mAP: {best_map_value:.4f})"
        )
        return best_checkpoint_path

    # best_mapì´ ì—†ìœ¼ë©´ ê°€ì¥ ìµœê·¼ íŒŒì¼ ì‚¬ìš©
    print("[AI] âš ï¸  best_map ì •ë³´ê°€ ì—†ì–´ ê°€ì¥ ìµœê·¼ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    best_checkpoint_path = max(checkpoint_files, key=os.path.getmtime)
    print(f"[AI] âœ… ì„ íƒëœ ëª¨ë¸: {os.path.basename(best_checkpoint_path)}")
    return best_checkpoint_path


def _setup_numpy_compatibility():
    """numpy ë²„ì „ í˜¸í™˜ì„± ë¬¸ì œ í•´ê²° (numpy._core -> numpy.core ë§¤í•‘)"""
    try:
        import numpy.core as numpy_core

        if "numpy._core" not in sys.modules:
            sys.modules["numpy._core"] = numpy_core
            sys.modules["numpy._core._multiarray_umath"] = numpy_core._multiarray_umath
    except Exception:
        pass  # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰


def _validate_model_file(model_path):
    """ëª¨ë¸ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ë° ë¬´ê²°ì„± ê²€ì‚¬"""
    if not os.path.exists(model_path):
        print(f"[AI] âŒ ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
        return False

    file_size = os.path.getsize(model_path)
    print(f"[AI] ëª¨ë¸ íŒŒì¼ í¬ê¸°: {file_size / (1024 * 1024):.2f} MB")

    # ZIP ì•„ì¹´ì´ë¸Œ ë¬´ê²°ì„± ê²€ì‚¬
    if model_path.endswith((".pth", ".pt")):
        try:
            with zipfile.ZipFile(model_path, "r") as z:
                z.testzip()
            print("[AI] âœ… ZIP ì•„ì¹´ì´ë¸Œ ë¬´ê²°ì„± ê²€ì‚¬ í†µê³¼")
        except zipfile.BadZipFile:
            print("[AI] âš ï¸  ZIP ì•„ì¹´ì´ë¸Œ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤ (ì •ìƒì¼ ìˆ˜ ìˆìŒ)")
        except Exception as e:
            print(f"[AI] âš ï¸  íŒŒì¼ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì†): {str(e)[:100]}")

    return True


def _load_checkpoint_with_fallback(model_path):
    """ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹œë„"""
    load_methods = [
        (
            "ê¸°ë³¸ ë°©ë²• (weights_only=False)",
            lambda: torch.load(model_path, map_location="cpu", weights_only=False),
        ),
        (
            "weights_only=True",
            lambda: torch.load(model_path, map_location="cpu", weights_only=True),
        ),
        ("íŒŒì¼ í•¸ë“¤ ì§ì ‘ ì‚¬ìš©", lambda: _load_with_file_handle(model_path)),
        ("pickle ì§ì ‘ ì‚¬ìš©", lambda: _load_with_pickle(model_path)),
        ("ì„ì‹œ íŒŒì¼ë¡œ ë³µì‚¬ í›„ ì¬ì‹œë„", lambda: _load_with_temp_file(model_path)),
    ]

    last_error = None
    for method_name, load_func in load_methods:
        try:
            print(f"[AI] ë¡œë“œ ì‹œë„: {method_name}")
            checkpoint = load_func()
            print(f"[AI] âœ… {method_name}ìœ¼ë¡œ ë¡œë“œ ì„±ê³µ!")
            return checkpoint
        except Exception as e:
            last_error = e
            error_msg = str(e)[:200]
            print(f"[AI] âš ï¸  {method_name} ì‹¤íŒ¨: {error_msg}")

    print("[AI] âŒ ëª¨ë“  ë¡œë“œ ë°©ë²• ì‹¤íŒ¨")
    if last_error:
        print(f"[AI] ë§ˆì§€ë§‰ ì˜¤ë¥˜: {last_error}")
    return None


def _load_with_file_handle(model_path):
    """íŒŒì¼ í•¸ë“¤ì„ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ë¡œë“œ"""
    with open(model_path, "rb") as f:
        return torch.load(f, map_location="cpu", weights_only=False)


def _load_with_pickle(model_path):
    """pickleì„ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ë¡œë“œ"""
    with open(model_path, "rb") as f:
        unpickler = pickle.Unpickler(f)
        unpickler.persistent_load = lambda pid: None  # persistent ID ë¬´ì‹œ
        return unpickler.load()


def _load_with_temp_file(model_path):
    """ì„ì‹œ íŒŒì¼ë¡œ ë³µì‚¬ í›„ ì¬ì‹œë„"""
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pth") as tmp_file:
        tmp_path = tmp_file.name
        try:
            shutil.copy2(model_path, tmp_path)
            checkpoint = torch.load(tmp_path, map_location="cpu", weights_only=False)
            return checkpoint
        finally:
            if os.path.exists(tmp_path):
                os.unlink(tmp_path)


def _extract_labels(checkpoint):
    """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë ˆì´ë¸” ì •ë³´ ì¶”ì¶œ"""
    if checkpoint.get("id2label"):
        num_classes = len(checkpoint["id2label"])
        id2label = checkpoint["id2label"]
    else:
        num_classes = checkpoint.get("num_classes", DEFAULT_NUM_CLASSES)
        id2label = {i: f"LABEL_{i}" for i in range(num_classes)}

    # í•œê¸€ ë ˆì´ë¸” ë§¤í•‘
    id2label_korean = {k: v for k, v in KOREAN_LABELS.items() if k < num_classes}

    return num_classes, id2label, id2label_korean


def _load_model_state(model, checkpoint):
    """ëª¨ë¸ì— ì²´í¬í¬ì¸íŠ¸ state_dict ë¡œë“œ"""
    if "model_state_dict" in checkpoint:
        state_dict = checkpoint["model_state_dict"]
        # float32ë¡œ ë³€í™˜
        for k, v in state_dict.items():
            if isinstance(v, torch.Tensor):
                state_dict[k] = v.to(torch.float32)
        model.model.load_state_dict(state_dict, strict=False)
    else:
        print(
            "[AI] âš ï¸  ì²´í¬í¬ì¸íŠ¸ì— 'model_state_dict' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì§ì ‘ ë¡œë“œ ì‹œë„..."
        )
        model.model.load_state_dict(checkpoint, strict=False)


def _print_model_info(checkpoint, num_classes, id2label, id2label_korean):
    """ëª¨ë¸ ì •ë³´ ì¶œë ¥"""
    print("[AI] âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
    print(f"[AI]    í´ë˜ìŠ¤ ìˆ˜: {num_classes}ê°œ")
    print(f"[AI]    ë ˆì´ë¸”: {id2label}")
    print(f"[AI]    í•œê¸€ ë ˆì´ë¸”: {id2label_korean}")

    if "epoch" in checkpoint:
        print(f"[AI]    Epoch: {checkpoint['epoch'] + 1}")
    if "best_map" in checkpoint:
        print(f"[AI]    Best mAP: {checkpoint['best_map']:.4f}")


def _select_device():
    """ìµœì ì˜ ë””ë°”ì´ìŠ¤ ì„ íƒ (CUDA -> CPU í´ë°±)"""
    if torch.cuda.is_available():
        try:
            # CUDA ë©”ëª¨ë¦¬ ì •ë¦¬
            torch.cuda.empty_cache()
            device = torch.device("cuda")
            print(f"[AI] âœ… CUDA ë””ë°”ì´ìŠ¤ ì‚¬ìš©: {torch.cuda.get_device_name(0)}")
            return device
        except Exception as e:
            print(f"[AI] âš ï¸  CUDA ì‚¬ìš© ì‹¤íŒ¨, CPUë¡œ í´ë°±: {e}")
    else:
        print("[AI] â„¹ï¸  CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    return torch.device("cpu")


def load_ai_model(max_retries=3, retry_delay=2):
    """
    AI ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)

    Args:
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        retry_delay: ì¬ì‹œë„ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
    """
    global model, processor, id2label, id2label_korean, resolved_model_path

    for attempt in range(max_retries):
        try:
            if attempt > 0:
                print(f"[AI] ğŸ”„ ëª¨ë¸ ë¡œë“œ ì¬ì‹œë„ {attempt}/{max_retries-1}...")
                import time

                time.sleep(retry_delay * attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„

            # ëª¨ë¸ ê²½ë¡œ ì°¾ê¸°
            model_path = _find_model_path()
            if not model_path:
                if attempt == max_retries - 1:
                    print("[AI] âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
                    print("[AI]    ë‹¤ìŒ ìœ„ì¹˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”:")
                    print("[AI]    1. í™˜ê²½ë³€ìˆ˜ MODEL_PATH ì„¤ì •")
                    print(f"[AI]    2. {AI_DIR}/ ë””ë ‰í† ë¦¬ì— .pt ë˜ëŠ” .pth íŒŒì¼ ë°°ì¹˜")
                    return False
                continue

            resolved_model_path = model_path
            print(f"[AI] ëª¨ë¸ íŒŒì¼ ë¡œë“œ ì¤‘: {model_path}")

            # íŒŒì¼ ê²€ì¦
            if not _validate_model_file(model_path):
                if attempt == max_retries - 1:
                    resolved_model_path = None
                    return False
                continue

            # numpy í˜¸í™˜ì„± ì„¤ì •
            _setup_numpy_compatibility()

            # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            checkpoint = _load_checkpoint_with_fallback(model_path)
            if checkpoint is None:
                if attempt == max_retries - 1:
                    file_size = os.path.getsize(model_path)
                    print("[AI] âš ï¸  ëª¨ë¸ íŒŒì¼ì´ ì†ìƒë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    print("[AI]    í•´ê²° ë°©ë²•:")
                    print("[AI]    1. ëª¨ë¸ íŒŒì¼ì„ ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œ/ë³µì‚¬í•˜ì„¸ìš”")
                    print("[AI]    2. íŒŒì¼ì´ ì™„ì „íˆ ì „ì†¡ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")
                    print(
                        f"[AI]    3. íŒŒì¼ í¬ê¸°ê°€ ì •ìƒì¸ì§€ í™•ì¸í•˜ì„¸ìš” (í˜„ì¬: {file_size / (1024*1024):.2f} MB)"
                    )
                    traceback.print_exc()
                    resolved_model_path = None
                    return False
                continue

            # ë ˆì´ë¸” ì •ë³´ ì¶”ì¶œ
            num_classes, id2label, id2label_korean = _extract_labels(checkpoint)

            # ë””ë°”ì´ìŠ¤ ì„ íƒ
            device = _select_device()

            # ëª¨ë¸ ì´ˆê¸°í™” ë° ë¡œë“œ
            try:
                model = CustomDeta(num_labels=num_classes)
                _load_model_state(model, checkpoint)

                # ëª¨ë¸ì„ ì„ íƒëœ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                model = model.to(device)
                model.eval()

                # ë©”ëª¨ë¦¬ ì •ë¦¬
                if device.type == "cuda":
                    torch.cuda.empty_cache()
            except RuntimeError as e:
                error_msg = str(e).lower()
                if "out of memory" in error_msg or "cuda" in error_msg:
                    print(f"[AI] âš ï¸  GPU ë©”ëª¨ë¦¬ ë¶€ì¡±, CPUë¡œ í´ë°± ì‹œë„...")
                    device = torch.device("cpu")
                    model = CustomDeta(num_labels=num_classes)
                    _load_model_state(model, checkpoint)
                    model = model.to(device)
                    model.eval()
                else:
                    raise

            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í”„ë¡œì„¸ì„œ ë¡œë“œ (ì¬ì‹œë„ í¬í•¨)
            processor = None
            for proc_attempt in range(3):
                try:
                    processor = DetaImageProcessor.from_pretrained(DETA_MODEL_NAME)
                    break
                except Exception as e:
                    if proc_attempt == 2:
                        raise
                    print(f"[AI] âš ï¸  í”„ë¡œì„¸ì„œ ë¡œë“œ ì‹¤íŒ¨, ì¬ì‹œë„ {proc_attempt+1}/3: {e}")
                    import time

                    time.sleep(1)

            if processor is None:
                raise RuntimeError("í”„ë¡œì„¸ì„œ ë¡œë“œ ì‹¤íŒ¨")

            # ëª¨ë¸ ì •ë³´ ì¶œë ¥
            _print_model_info(checkpoint, num_classes, id2label, id2label_korean)
            print(f"[AI] âœ… ëª¨ë¸ì´ {device}ì— ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")

            return True

        except Exception as e:
            error_msg = str(e)
            print(
                f"[AI] âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ (ì‹œë„ {attempt+1}/{max_retries}): {error_msg}"
            )
            if attempt == max_retries - 1:
                traceback.print_exc()
                model, processor, id2label, id2label_korean, resolved_model_path = (
                    None,
                    None,
                    None,
                    None,
                    None,
                )
                return False
            # ë‹¤ìŒ ì‹œë„ë¥¼ ìœ„í•´ ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™”
            model, processor, id2label, id2label_korean = None, None, None, None

    return False


def get_model():
    """í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ ë°˜í™˜"""
    return model


def get_processor():
    """í˜„ì¬ ë¡œë“œëœ í”„ë¡œì„¸ì„œ ë°˜í™˜"""
    return processor


def get_id2label():
    """í˜„ì¬ ë¡œë“œëœ ë ˆì´ë¸” ë§µ ë°˜í™˜"""
    return id2label


def get_id2label_korean():
    """í˜„ì¬ ë¡œë“œëœ í•œê¸€ ë ˆì´ë¸” ë§µ ë°˜í™˜"""
    return id2label_korean


def is_model_loaded():
    """ëª¨ë¸ ë¡œë“œ ì—¬ë¶€ í™•ì¸"""
    return model is not None and processor is not None


def get_resolved_model_path():
    """ë§ˆì§€ë§‰ìœ¼ë¡œ ë¡œë“œì— ì‚¬ìš©ëœ ëª¨ë¸ ê²½ë¡œ ë°˜í™˜"""
    return resolved_model_path


def resolve_model_path():
    """í˜„ì¬ ì„¤ì •ì—ì„œ íƒìƒ‰ëœ ëª¨ë¸ ê²½ë¡œ ë°˜í™˜ (ë¡œë“œ ì—†ì´)"""
    return _find_model_path()
